{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"About Getting Started Example of Math Formula \\[\\mathbb{P}=\\mathbb{NP}\\] Example of Click Box Item that is ready Item not ready Example of Content Box C++ // Example of C++ Code int main () { return 0; } python # Example of python Code if __name__ == \"__main__\": pass Example of Pop-out Notice Important Notice All the changes","title":"About"},{"location":"index.html#about","text":"","title":"About"},{"location":"index.html#getting-started","text":"","title":"Getting Started"},{"location":"index.html#example-of-math-formula","text":"\\[\\mathbb{P}=\\mathbb{NP}\\]","title":"Example of Math Formula"},{"location":"index.html#example-of-click-box","text":"Item that is ready Item not ready","title":"Example of Click Box"},{"location":"index.html#example-of-content-box","text":"C++ // Example of C++ Code int main () { return 0; } python # Example of python Code if __name__ == \"__main__\": pass","title":"Example of Content Box"},{"location":"index.html#example-of-pop-out-notice","text":"Important Notice All the changes","title":"Example of Pop-out Notice"},{"location":"Big_Data/index.html","text":"Distrubuted System Overview Course side: 1. Understand the new issues appearing as datasets grow 2. Be able to setup a Hadoop cluster and use it 3. Understand why traditional algorithms fail on big data 4. Be able to implement advanced algorithms for big data Personnal side: 1. Derive algorithms for big data 2. Use and work \u201cinside\u201d Hadoop, Drill, and Spark 3. Relate known strategies to new problems 4. Perform extra research Topic 1: New issues for Big data Big Data Issues (1) increase throughput: caching, branch prediction, RAID (2) size of data size: >1T (old definition) Hadoop/Drill/Spark/... Goal: efficiently analyse massive amount of data Hadoop: batch job , latency not important, write on disk Drill: real-time job, sql friendly Spark: real-time job, good analytics Zookeeper Configuration Management & Coordination Service. - Dependent of Drill. - No large data-store, allow different nodes in cluster to communicate; - let various applications of hadoop to work together. Node communication (like dbus) Leader/Follower/Observer YARN \"Yet Another Resource Negotiator\" Resource Manager (whole system) Node Manager (container) Application Manager (application) YARN: 3 working options 3 ways of using YARN: 1. One application per user job (MapReduce) 2. One application per user session (Spark) 3. Long-running application shared among users (Drill) YARN scheduler 3 scheduler mode 1. FIFO: good for batch 2. Capacity: waster resource 3. Fair: delay due to the resource reallocation Example Total: 100 CPU, 10 TB - A: 2 CPU, 300 GB - (2%, 3%) - 3% - B: 6 CPU, 200 GB - (6%, 1%) - 6% if A have 10 containers, B will have 5 containers. YARN: workflow client request Resource manager (return assigned id) request a Node to run Application Master (assigned by Resource Manager->Node Manager, return id) Application Master register itself to Resource Manager (later it can apply for resource) Application Master request containers from the Resource Manager Application Master inform the Node Manager to do the initialization Application Master monitor the Running status of all the Node Managers Application Master send heartbeat to the client, report the process Application Manager undo the register on Resource Manager, Resource Manager shut down the Application Master and stop the tasks, free all the Node Manager. Hadoop: MapReduce fetch file information client submit job to YARN YARN calculate the resource, and start the MapTasks Read data with gradually increased offset value YARN starts Map Task (create a Map Tasks for each split) Ring Buffer (>80%) Shuffle (quick sort) Shuffle (merge sort) Local Reducer YARN starts Reduce Task (depends on Map Tasks number) YARN fetch data to memory (+disk) Reduce, output Key Value pair Write back to hdfs HDFS: merge small files why? 1. the memory of namenode: meta data 2. the latency of fetching too many small files 3. timecost of starting too many tasks solutions: 1. Hadoop archive (HAR): A MapReduce Task 2. SequenceFile / TextFile (Filecrush/Avro): serialize <key, value> pair where key is the filename and value is the file content. 3. CombineFileInputFormat HDFS: Failure Tolarence HDFS: datanode failure datanode send heartbeat to namenode every 3 sec HDFS: network failure datanode does not respond the request HDFS: Data failure checksum, data lost if network is unstable, read from other data node HDFS: general Block Size is 128MB by default Task Size is 128MB by default Pro: high throughput Con: large latency Inputformat: (1) split big files to blocks (2) merge small files Container A container is an environment with restricted resources where application-specific processes are run. JAVA JVM: JAVA: cross-platform - Default MapReduce Format: LongWritable, TEXT. public class MyMapper extends Mapper<LongWritable, Text, Text, IntWritable>{ @Override public void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] data = value.toString().split(\",\"); Text id = new Text(data[1]); IntWritable grade = new IntWritable(Integer.parseInt(data[2])); context.write(id, grade); } } maven mvn package mvn compile OS namespace: security. hierarchical management of processes isolation Topic 2: Hadoop cluster setup, what is inside Hadoop, Drill and Spark MapReduce Spill spill happens when the memory is not enough. It will write the data to the disk. For MapReduce1.0, spilling will happen at least once because the output of the Mapper will be written to the disk. speed up a MapReduce Job Provide shuffle with as much memory as possible Keep enough memory for map and reduce functions Optimize the code with respect to memory consumption Minimize the number of spills for the map part. Keep intermediate reduce data in memory Drill Parties optionally involved in a Drill job: 1. YARN 2. HDFS 3. Hive 4. HBase - Drill-on-Yarn: Running Drill as a YARN application - Drillbit: A Drill Process - Foreman Step 1: Start Drill on the client machine drill-on-yarn.sh --site $DRILL_SITE [start/stop/status] Step 2: Upload resources to the FS and request resources for the application master Since we are running drill as a YARN application, we have to upload all the jars. drill.yarn: drill-install: client-path: \"/opt/drill_master/drill.tar.gz\" drill.yarn: dfs: app-dir: \"/user/drill\" Step 3: Ask a node manager to prepare and start a container for the application master drill.yarn: http: port: 8048 # the admin web service Step 4: The application master contacts the resource manager to obtain more containers drill.yarn: cluster: count: 3 # Drill-on-YARN runs each on a seperate hosts Step 5: Request the start of Drill software on each assigned node Drill will have a drill-master, where the application master is. Step 6: Start a \"Drill process\" called a drillbit Drillbit is a process, drill is an application, handles queries Step 7: Each drillbit starts and registers with Zookeeper drill.exec: zk.connect: \"59.78.36.36:2181,59.78.35.67:2181,59.78.36.88:2181\" # list of zookeeper hosts Step 8: The application master checks the health of each drillbit through Zookeeper Each datanode should configure the zk (align with drillbit) tickTime=2000 dataDir=/opt/zookeeper/data clientPort=2181 initLimit=5 syncLimit=2 server.3=59.78.36.36:2888:3888 server.2=59.78.36.88:2888:3888 server.1=59.78.35.67:2888:3888 Step 9: Use Zookeeper to retrieve information on the drillbits, run queries, etc. Drillbit Foreman drillbit: the drillbit that receives the query (each drillbit has a service deployed at port 8048 and 8047, which can receive queries from the user.) Foreman drillbit is responsible for driving the whole query: from SQL, logical plan, Optimizer, Physical Plan, Execution, Storage(if update) Each drillbit contains all services and capabilities of Drill Columnar execution Optimistic query execution\uff1a (1) assume queries will success (2) re-run failed queries (3) never store intermediate results (only if memory is run out of) Vectorization Runtime Compilation Drill Fragmentation The physical plan is an execution tree with multiple fragments (JSON format). Major fragments: Minor fragments: - Root Minor fragments: run the foreman - Intermediate Minor fragments: pass aggregated results to the root fragment - Leaf Minor fragments: parallel data fetching Spark About RDD RDD: Resilient Distributed Dataset (1) Read-only (2) Partitioned fork and join optimization Spark DAG Scheduler - Resilient: reconstruct the RDD based on partition loss Run spark we only need to set the spark variables export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop export SPARK_HOME=\"/usr/local/spark\" export PATH=$PATH:$SPARK_HOME/bin export LD_LIBRARY_PATH=${HADOOP_HOME}/lib/native export SPARK_DIST_CLASSPATH=$(hadoop classpath) Spark running mode: client mode: the process is running on the master (for interactive) cluster mode: the process is running as a YARN application (fully utilize hadoop) Why is Spark fast Distributed collections of object that can be cached in memory Manipulated through various parallel operations Automatically rebuild on failure Spark RDD optimization Transformation Using lazy evaluation (not executed until it sees an action, reorganize and optimize the process \"fork and join\"). avoid returning large datasets. Narrow Transformations rdd.map(lambda x:x+1) rdd.filter() rdd.flatmap() wide transforamtions rdd.join() rdd.cartesian() rdd.groupbykey() rdd.reducebykey() import pyspark.sql as sql spark = sql.session.SparkSession.builder.master(\"yarn\").appName(\"test\").getOrCreate() grade_rdd = spark.read.text(\"hdfs://hadoop-master:9000/user/hadoop/\" + str(row_count) + \".csv\").rdd grade_pairs = grade_rdd.flatMap(lambda row: [tuple(row.value.split(\",\")[1:3])]) max_grade = grade_pairs.reduceByKey(lambda x, y: max([x, y])) max_grade.collect() Action rdd.collect() LXC Linux Container Docker based on LXC Kubernetes organize small cluster of containers when to use what? ~5GB: powerful personal computer ~100GB: Kubernetes ~1PB: hadoop LVM Logical Volumn Manager: manage filesysten disk partition RPC Remote Procedure Call: contact Name Node","title":"Distrubuted System Overview"},{"location":"Big_Data/index.html#distrubuted-system-overview","text":"Course side: 1. Understand the new issues appearing as datasets grow 2. Be able to setup a Hadoop cluster and use it 3. Understand why traditional algorithms fail on big data 4. Be able to implement advanced algorithms for big data Personnal side: 1. Derive algorithms for big data 2. Use and work \u201cinside\u201d Hadoop, Drill, and Spark 3. Relate known strategies to new problems 4. Perform extra research","title":"Distrubuted System Overview"},{"location":"Big_Data/index.html#topic-1-new-issues-for-big-data","text":"Big Data Issues (1) increase throughput: caching, branch prediction, RAID (2) size of data size: >1T (old definition) Hadoop/Drill/Spark/... Goal: efficiently analyse massive amount of data Hadoop: batch job , latency not important, write on disk Drill: real-time job, sql friendly Spark: real-time job, good analytics Zookeeper Configuration Management & Coordination Service. - Dependent of Drill. - No large data-store, allow different nodes in cluster to communicate; - let various applications of hadoop to work together. Node communication (like dbus) Leader/Follower/Observer YARN \"Yet Another Resource Negotiator\" Resource Manager (whole system) Node Manager (container) Application Manager (application) YARN: 3 working options 3 ways of using YARN: 1. One application per user job (MapReduce) 2. One application per user session (Spark) 3. Long-running application shared among users (Drill) YARN scheduler 3 scheduler mode 1. FIFO: good for batch 2. Capacity: waster resource 3. Fair: delay due to the resource reallocation Example Total: 100 CPU, 10 TB - A: 2 CPU, 300 GB - (2%, 3%) - 3% - B: 6 CPU, 200 GB - (6%, 1%) - 6% if A have 10 containers, B will have 5 containers. YARN: workflow client request Resource manager (return assigned id) request a Node to run Application Master (assigned by Resource Manager->Node Manager, return id) Application Master register itself to Resource Manager (later it can apply for resource) Application Master request containers from the Resource Manager Application Master inform the Node Manager to do the initialization Application Master monitor the Running status of all the Node Managers Application Master send heartbeat to the client, report the process Application Manager undo the register on Resource Manager, Resource Manager shut down the Application Master and stop the tasks, free all the Node Manager. Hadoop: MapReduce fetch file information client submit job to YARN YARN calculate the resource, and start the MapTasks Read data with gradually increased offset value YARN starts Map Task (create a Map Tasks for each split) Ring Buffer (>80%) Shuffle (quick sort) Shuffle (merge sort) Local Reducer YARN starts Reduce Task (depends on Map Tasks number) YARN fetch data to memory (+disk) Reduce, output Key Value pair Write back to hdfs HDFS: merge small files why? 1. the memory of namenode: meta data 2. the latency of fetching too many small files 3. timecost of starting too many tasks solutions: 1. Hadoop archive (HAR): A MapReduce Task 2. SequenceFile / TextFile (Filecrush/Avro): serialize <key, value> pair where key is the filename and value is the file content. 3. CombineFileInputFormat HDFS: Failure Tolarence","title":"Topic 1: New issues for Big data"},{"location":"Big_Data/index.html#hdfs-datanode-failure","text":"datanode send heartbeat to namenode every 3 sec","title":"HDFS: datanode failure"},{"location":"Big_Data/index.html#hdfs-network-failure","text":"datanode does not respond the request","title":"HDFS: network failure"},{"location":"Big_Data/index.html#hdfs-data-failure","text":"checksum, data lost if network is unstable, read from other data node HDFS: general Block Size is 128MB by default Task Size is 128MB by default Pro: high throughput Con: large latency Inputformat: (1) split big files to blocks (2) merge small files Container A container is an environment with restricted resources where application-specific processes are run. JAVA JVM: JAVA: cross-platform - Default MapReduce Format: LongWritable, TEXT. public class MyMapper extends Mapper<LongWritable, Text, Text, IntWritable>{ @Override public void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] data = value.toString().split(\",\"); Text id = new Text(data[1]); IntWritable grade = new IntWritable(Integer.parseInt(data[2])); context.write(id, grade); } } maven mvn package mvn compile OS namespace: security. hierarchical management of processes isolation","title":"HDFS: Data failure"},{"location":"Big_Data/index.html#topic-2-hadoop-cluster-setup-what-is-inside-hadoop-drill-and-spark","text":"MapReduce Spill spill happens when the memory is not enough. It will write the data to the disk. For MapReduce1.0, spilling will happen at least once because the output of the Mapper will be written to the disk. speed up a MapReduce Job Provide shuffle with as much memory as possible Keep enough memory for map and reduce functions Optimize the code with respect to memory consumption Minimize the number of spills for the map part. Keep intermediate reduce data in memory Drill Parties optionally involved in a Drill job: 1. YARN 2. HDFS 3. Hive 4. HBase - Drill-on-Yarn: Running Drill as a YARN application - Drillbit: A Drill Process - Foreman","title":"Topic 2: Hadoop cluster setup, what is inside Hadoop, Drill and Spark"},{"location":"Big_Data/index.html#step-1-start-drill-on-the-client-machine","text":"drill-on-yarn.sh --site $DRILL_SITE [start/stop/status]","title":"Step 1: Start Drill on the client machine"},{"location":"Big_Data/index.html#step-2-upload-resources-to-the-fs-and-request-resources-for-the-application-master","text":"Since we are running drill as a YARN application, we have to upload all the jars. drill.yarn: drill-install: client-path: \"/opt/drill_master/drill.tar.gz\" drill.yarn: dfs: app-dir: \"/user/drill\"","title":"Step 2: Upload resources to the FS and request resources for the application master"},{"location":"Big_Data/index.html#step-3-ask-a-node-manager-to-prepare-and-start-a-container-for-the-application-master","text":"drill.yarn: http: port: 8048 # the admin web service","title":"Step 3: Ask a node manager to prepare and start a container for the application master"},{"location":"Big_Data/index.html#step-4-the-application-master-contacts-the-resource-manager-to-obtain-more-containers","text":"drill.yarn: cluster: count: 3 # Drill-on-YARN runs each on a seperate hosts","title":"Step 4: The application master contacts the resource manager to obtain more containers"},{"location":"Big_Data/index.html#step-5-request-the-start-of-drill-software-on-each-assigned-node","text":"Drill will have a drill-master, where the application master is.","title":"Step 5: Request the start of Drill software on each assigned node"},{"location":"Big_Data/index.html#step-6-start-a-drill-process-called-a-drillbit","text":"Drillbit is a process, drill is an application, handles queries","title":"Step 6: Start a \"Drill process\" called a drillbit"},{"location":"Big_Data/index.html#step-7-each-drillbit-starts-and-registers-with-zookeeper","text":"drill.exec: zk.connect: \"59.78.36.36:2181,59.78.35.67:2181,59.78.36.88:2181\" # list of zookeeper hosts","title":"Step 7: Each drillbit starts and registers with Zookeeper"},{"location":"Big_Data/index.html#step-8-the-application-master-checks-the-health-of-each-drillbit-through-zookeeper","text":"Each datanode should configure the zk (align with drillbit) tickTime=2000 dataDir=/opt/zookeeper/data clientPort=2181 initLimit=5 syncLimit=2 server.3=59.78.36.36:2888:3888 server.2=59.78.36.88:2888:3888 server.1=59.78.35.67:2888:3888","title":"Step 8: The application master checks the health of each drillbit through Zookeeper"},{"location":"Big_Data/index.html#step-9-use-zookeeper-to-retrieve-information-on-the-drillbits-run-queries-etc","text":"Drillbit Foreman drillbit: the drillbit that receives the query (each drillbit has a service deployed at port 8048 and 8047, which can receive queries from the user.) Foreman drillbit is responsible for driving the whole query: from SQL, logical plan, Optimizer, Physical Plan, Execution, Storage(if update) Each drillbit contains all services and capabilities of Drill Columnar execution Optimistic query execution\uff1a (1) assume queries will success (2) re-run failed queries (3) never store intermediate results (only if memory is run out of) Vectorization Runtime Compilation Drill Fragmentation The physical plan is an execution tree with multiple fragments (JSON format). Major fragments: Minor fragments: - Root Minor fragments: run the foreman - Intermediate Minor fragments: pass aggregated results to the root fragment - Leaf Minor fragments: parallel data fetching Spark","title":"Step 9: Use Zookeeper to retrieve information on the drillbits, run queries, etc."},{"location":"Big_Data/index.html#about-rdd","text":"RDD: Resilient Distributed Dataset (1) Read-only (2) Partitioned fork and join optimization Spark DAG Scheduler - Resilient: reconstruct the RDD based on partition loss","title":"About RDD"},{"location":"Big_Data/index.html#run-spark","text":"we only need to set the spark variables export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop export SPARK_HOME=\"/usr/local/spark\" export PATH=$PATH:$SPARK_HOME/bin export LD_LIBRARY_PATH=${HADOOP_HOME}/lib/native export SPARK_DIST_CLASSPATH=$(hadoop classpath)","title":"Run spark"},{"location":"Big_Data/index.html#spark-running-mode","text":"client mode: the process is running on the master (for interactive) cluster mode: the process is running as a YARN application (fully utilize hadoop)","title":"Spark running mode:"},{"location":"Big_Data/index.html#why-is-spark-fast","text":"Distributed collections of object that can be cached in memory Manipulated through various parallel operations Automatically rebuild on failure Spark RDD optimization","title":"Why is Spark fast"},{"location":"Big_Data/index.html#transformation","text":"Using lazy evaluation (not executed until it sees an action, reorganize and optimize the process \"fork and join\"). avoid returning large datasets.","title":"Transformation"},{"location":"Big_Data/index.html#narrow-transformations","text":"rdd.map(lambda x:x+1) rdd.filter() rdd.flatmap()","title":"Narrow Transformations"},{"location":"Big_Data/index.html#wide-transforamtions","text":"rdd.join() rdd.cartesian() rdd.groupbykey() rdd.reducebykey() import pyspark.sql as sql spark = sql.session.SparkSession.builder.master(\"yarn\").appName(\"test\").getOrCreate() grade_rdd = spark.read.text(\"hdfs://hadoop-master:9000/user/hadoop/\" + str(row_count) + \".csv\").rdd grade_pairs = grade_rdd.flatMap(lambda row: [tuple(row.value.split(\",\")[1:3])]) max_grade = grade_pairs.reduceByKey(lambda x, y: max([x, y])) max_grade.collect()","title":"wide transforamtions"},{"location":"Big_Data/index.html#action","text":"rdd.collect() LXC Linux Container Docker based on LXC Kubernetes organize small cluster of containers when to use what? ~5GB: powerful personal computer ~100GB: Kubernetes ~1PB: hadoop LVM Logical Volumn Manager: manage filesysten disk partition RPC Remote Procedure Call: contact Name Node","title":"Action"},{"location":"Logic_Synthesis/index.html","text":"About Logic Synthesis Chapter 1 Introduction of Boolean Algebra Section 1: Boolean Axioms Chapter 2 Logic Modeling Section 1: Truth Table Section 2: Binary Decision Diagram Section 3: Satisfiability Chapter 3 Area Optimization Section 1: Rewrite Section 2: Refactor Section 3: Resubstitution Chapter 4 Delay Optimization Section 1: Balance Section 2: Gate Duplication Section 3: Buffer Insertion Chapter 5 Runtime Speedup Section 1: Boolean Filter Section 2: Window Extraction","title":"About"},{"location":"Logic_Synthesis/index.html#about","text":"Logic Synthesis Chapter 1","title":"About"},{"location":"Logic_Synthesis/index.html#introduction-of-boolean-algebra","text":"Section 1: Boolean Axioms Chapter 2","title":"Introduction of Boolean Algebra"},{"location":"Logic_Synthesis/index.html#logic-modeling","text":"Section 1: Truth Table Section 2: Binary Decision Diagram Section 3: Satisfiability Chapter 3","title":"Logic Modeling"},{"location":"Logic_Synthesis/index.html#area-optimization","text":"Section 1: Rewrite Section 2: Refactor Section 3: Resubstitution Chapter 4","title":"Area Optimization"},{"location":"Logic_Synthesis/index.html#delay-optimization","text":"Section 1: Balance Section 2: Gate Duplication Section 3: Buffer Insertion Chapter 5","title":"Delay Optimization"},{"location":"Logic_Synthesis/index.html#runtime-speedup","text":"Section 1: Boolean Filter Section 2: Window Extraction","title":"Runtime Speedup"},{"location":"Logic_Synthesis/boolean.html","text":"Introduction of Boolean Algebra Boolean Axioms Delay Optimization: \\(x(yz) = (xy)z\\) Area Optimization: \\(x(y+z) = xy+xz\\) Trivial Optimization: \\(xx=x\\) , \\(xx'=0\\)","title":"Introduction of Boolean Algebra"},{"location":"Logic_Synthesis/boolean.html#introduction-of-boolean-algebra","text":"","title":"Introduction of Boolean Algebra"},{"location":"Logic_Synthesis/boolean.html#boolean-axioms","text":"Delay Optimization: \\(x(yz) = (xy)z\\) Area Optimization: \\(x(y+z) = xy+xz\\) Trivial Optimization: \\(xx=x\\) , \\(xx'=0\\)","title":"Boolean Axioms"},{"location":"Logic_Synthesis/ABC/ABC_Commands.html","text":"","title":"ABC Commands"},{"location":"Personal/linux.html","text":"My Linux Configurations In this year, I tried to deploy my linux OS on ubuntu 20.04 LTS. Below is the procedure, commands and scripts. Advantage of Ubuntu 20.04 LTS Comparing to 18.04 LTS: Install Linux: Some hardward will raise problem while sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo ubuntu-drivers devices Then we select the recommanded GPU driver version sudo apt-get install nvidia-driver-460 nvidia-settings nvidia-prime Step 0: Install ssh and sshd : By default, you should have ssh already in the environment. To install sshd , the server, run: sudo apt-get install openssh-server Then start the server: service sshd start Then we modify the file in /etc/ssh/sshd_config , and change the following settings: PubkeyAuthentication yes PasswordAuthentication no Next, put the public key of your PC to the ~/.ssh/authorized_keys so that we can log in without password. Then we run ssh and login remotely. frp Server: nohup ./frps -c frps.ini Client: nohup ./frpc -c frpc.ini Step 1: Install zsh and oh-my-zsh : First we install the dependencies, including zsh . sudo apt-get install curl wget git zsh Then we install oh-my-zsh from github: sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" And then change the theme of oh my zsh git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k And in ~/.zshrc , set powerlevel10k/powerlevel10k . zsh plugins auto suggestions git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Add the plugins zsh-autosuggestions in ~/.zshrc Step 2: Proxy Auto Config sudo apt-get install python3-pip sudo apt-get install shadowsocks-libev Then edit the file /etc/shadowsocks-libev/config.json then: sudo systemctl start shadowsocks-libev-local@config.service sudo systemctl enable shadowsocks-libev-local@config.service to check the status: netstat -tlnp proxy export https_proxy=\"socks5://127.0.0.1:1080\" Nginx PAC file generation sudo apt install nginx sudo pip install genpac sudo genpac --pac-proxy \"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-url=https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt --output=\"/var/www/html/autoproxy.pac\" set GNOME global settings gsettings set org.gnome.system.proxy mode auto gsettings set org.gnome.system.proxy autoconfig-url 'http://localhost/autoproxy.pac' To disable gsettings set org.gnome.system.proxy mode none","title":"My Linux Configurations"},{"location":"Personal/linux.html#my-linux-configurations","text":"In this year, I tried to deploy my linux OS on ubuntu 20.04 LTS. Below is the procedure, commands and scripts. Advantage of Ubuntu 20.04 LTS Comparing to 18.04 LTS:","title":"My Linux Configurations"},{"location":"Personal/linux.html#install-linux","text":"Some hardward will raise problem while sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo ubuntu-drivers devices Then we select the recommanded GPU driver version sudo apt-get install nvidia-driver-460 nvidia-settings nvidia-prime","title":"Install Linux:"},{"location":"Personal/linux.html#step-0-install-ssh-and-sshd","text":"By default, you should have ssh already in the environment. To install sshd , the server, run: sudo apt-get install openssh-server Then start the server: service sshd start Then we modify the file in /etc/ssh/sshd_config , and change the following settings: PubkeyAuthentication yes PasswordAuthentication no Next, put the public key of your PC to the ~/.ssh/authorized_keys so that we can log in without password. Then we run ssh and login remotely.","title":"Step 0: Install ssh and sshd:"},{"location":"Personal/linux.html#frp","text":"Server: nohup ./frps -c frps.ini Client: nohup ./frpc -c frpc.ini","title":"frp"},{"location":"Personal/linux.html#step-1-install-zsh-and-oh-my-zsh","text":"First we install the dependencies, including zsh . sudo apt-get install curl wget git zsh Then we install oh-my-zsh from github: sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" And then change the theme of oh my zsh git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k And in ~/.zshrc , set powerlevel10k/powerlevel10k .","title":"Step 1: Install zsh and oh-my-zsh:"},{"location":"Personal/linux.html#zsh-plugins","text":"","title":"zsh plugins"},{"location":"Personal/linux.html#auto-suggestions","text":"git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Add the plugins zsh-autosuggestions in ~/.zshrc","title":"auto suggestions"},{"location":"Personal/linux.html#step-2-proxy-auto-config","text":"sudo apt-get install python3-pip sudo apt-get install shadowsocks-libev Then edit the file /etc/shadowsocks-libev/config.json then: sudo systemctl start shadowsocks-libev-local@config.service sudo systemctl enable shadowsocks-libev-local@config.service to check the status: netstat -tlnp","title":"Step 2: Proxy Auto Config"},{"location":"Personal/linux.html#proxy","text":"export https_proxy=\"socks5://127.0.0.1:1080\"","title":"proxy"},{"location":"Personal/linux.html#nginx-pac-file-generation","text":"sudo apt install nginx sudo pip install genpac sudo genpac --pac-proxy \"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-url=https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt --output=\"/var/www/html/autoproxy.pac\"","title":"Nginx PAC file generation"},{"location":"Personal/linux.html#set-gnome-global-settings","text":"gsettings set org.gnome.system.proxy mode auto gsettings set org.gnome.system.proxy autoconfig-url 'http://localhost/autoproxy.pac' To disable gsettings set org.gnome.system.proxy mode none","title":"set GNOME global settings"},{"location":"Personal/research_summary.html","text":"Logic Synthesis Research Summary Timeline & Overview Declaration : useful work : not work 2019 Oct Read ABC base Code: understand the data structures inside ABC, e.g. Abc_Ntk_t , Abc_Obj_t , Abc_Cut_t , Vec_Ptr_t , ... Read BLIF , AIGER format: understand how to use tools to convert between different format: e.g. aig2aag , aag2aig Read EPFL benchmarks: understand the contest of best depth and best size . Write a python BLIF parser 2019 Dec Read graphviz : visualize the logic network based on dot and graphviz , can be then converted to post script format. Write Idea 1 . 2020 Mar Help Zhou Zhuoer Write Idea 2 2020 Jun Help Mao Xingyun Write Idea 3 Read SAT based algorithms, RL algorithms: understand recent efforts to apply machine learning algorithms on improving logic synthesis. Read Yosys and EPFL LS Library : parse verilog files and convert to BLIF files. learn EPFL toolkits, including Alice , Mockturtles and try ALSO which is implemented based on EPFL toolkit. Moreover, visit Open Source EDA tools on OpenRoad . 2020 Jul Read ABC advanced commands dc2 and dch . understand 3 basic operations in logic synthesis are rewrite , refactor and resub Write Idea 4 2020 Oct Read Stochastic Algorithms studied by Fiser , including paritial network , Cartesian Genetic Programming , and Random Permuation . Read LEKO and LEKU from J. Cong , which studies the optimality of logic synthesis. Write ABC Rewrite Pre-literature Search Report, which concludes the logic synthesis as a combinatorial optimization problem, which includes local optimizations and global scheduling. Write Visualization Engine for Capstone project, which visualize the logic rewriting process. 2020 Dec Write Idea 5 Read GNN algorithms 2021 Jan Read verification algorithms. understand how to combine simulation and SAT solver and the relationship between logic synthesis and verifications. Read Fanout Optimization Related Works. including TechMap Algorithms , Gate Duplciation , Buffer Insersion , and Rewiring algorithms. Help Huang Shen 2021 Apr Write Idea 6 2021 Jun Problems, Solutions, & Results Problem 1: Fanout Optimization with strict limit (2019.10 - 2020.03) Develop an algorithm to fix fanout overload in LUT network. The fanout limit for emerging circuit is low but current designs have large fanuot nodes, e.g. dec_depth_2018 's maximum fanout number is 64. Keywords: fanout Solution: convert local LUT network \\(N\\) to AIG \\(N'\\) . fix fanout number using buffer or duplication, get \\(N''\\) map to LUTs \\(N'''\\) Results: our method works on the network with small fanouts local network (MFFC) is usually big, sometimes the local extracted network is equivalent to the whole network. ABC mapper will run redundancy removal and revert our operations to fix fanout overload. Problem 2: Fanout Optimization with soft limit (2020.03 - 2020.04) Develop an algorithm to reduce the fanout delay on the critical path. The delay model is given by \\(Delay(G) = A+B\\times fanout(G))\\) . Keywords: fanout, delay optimization Solution: MFS (2020.03) Modify the target function in ABC mfs , find the new divisors to represent a function and optimize the fanout distribution upon nodes. On the other hand, if the area is optimized during mfs , new nodes are added to duplicate the high fanout nodes. mfs pseudocode for node in network: for n_old in node.fanin: n_new <= TryResub(node, n_old) if fanout(n_old) < fanout (n_new): Update(node): n_old => n_new duplication pseudocode _ntk <= MFS(ntk) area_saved <= node_num(ntk)-node_num(_ntk) while area_saved > 0: Duplicate(highest_fanout_node) area_saved-- Result: 18% TFCP reduction on average (on selected EPFL benchmarks, whose network is large enough and has room of improvement.) all of the reduction are caused by the area optimization and the duplication, but not our \"modified\" mfs method. Update: critical path update & larger window size (2020.11) Improve the duplication and mfs performance by updating the critical path after each network rewriting. Manually set the hyperparameters of ABC mfs function and increase the default window size of mfs while area optimization and rewiring. Result 28% (previous +10%) TFP reduction on average. Update: refine rewiring criteria & enable area increasement (2021.04) write the project ReFO and provide more options to the user. optimize the rewiring criteria and avoid making negative fanout changes to the network. Result 35% Delay Optimization ( \\(1+0.2\\times fanout(G)\\) model) without Area increase on average. Problem 3: Improve ABC rewrite performance on area optimization (2021.03 - present) The target of ABC rewrite is to reduce the AND nodes number in the AIG without increasing the level of it. There are several aspects that could be potentially optimized (sub-problems): 1. the combination of rewrite , refactor , resub . 2. the node traversal order and rewrite task scheduling of rewrite function. 3. the local 4-feasible cut rewrite machanism. Note that the 3 problems are listed from the upper level to the lower level. keywords: AIG, area optimization Solution: optimize the combination of rewrite , refactor , resub -- Priority Scheduler (2020.08) keywords: priority scheduler Add a filter the operation procedure and make decision for each node if it should be operated based on the evaluation result. priority scheduler pseudocode seq <= compress2rs sequence for op in seq: next_op <= op.next for node in network: gain <= EvalGain(op, node)-EvalGain(next_op, node) if gain > 0: Run(op, node) EvalGain(op, node) means try operation op (one of the rewrite , refactor , rewrite ) on the node , and return the gain (node number reduction for area optimization) can get from this operation. We only run operation on this node if and only if the current operation is better than the later operation. Result 0.1% area improvement on the EPFL benchmark hard to decide which solution is better (although the target does not consider level, ABC rewrite is able to reduce the delay. Sometime our area is better but delay is worse). Solution: the node traversal order and rewrite task scheduling of rewrite function (2020.03 - 2020.08) keywords: simulated annealing Modify the traversal order and rewrite update decision making by including a simulated annealing process into the rewrite engine. simulated annealing pseudocode T <= T_start # initial temperature while T > T_end: node <= RandomPick(network) op <= RandomPick({rewrite,refactor,resub}) gain <= Eval(op, node) with exp(gain/T) as probability: Run(op, node) T <= CoolDown(T) Result converge to the same result no matter how we set the parameters. the probability of getting \"coincidence\" that \"accidently\" negatively change the neighbour nodes in the correct way is extremely low (the probability is equal to evolutionary algorithms like CGP). do not have enough theoretical proofs of our negative moves. Update: ALTERSEQ (2020.05) keywords: alternately In order to increase the probability of getting useful negative rewriting and make more changes to the current state, we alternately run Negative rewriting and Positive rewriting, which is similar to the Espresso expand and reduce operations. ALTERNEQ pseudocode iter <= num_iterations # set the number of iterations for iter times: # negative rewrite nodes <= RandomPick(network, num_nodes) NegRewrite(nodes) # positive rewrite PosRewrite(network) Reuslt found some useful negative rewrite on one trail but the probability is still low. the runtime is long due to the large iteration numbers (we have to make sure the value of num_nodes*num_iterations is about the number of nodes we have) Update: QUICKSEQ (2020.06) keywords: quick To accelarate previous method, we develop a faster script. QUICKSEQ pseudocode iter <= num_iterations # set the number of iterations for iter times: # negative rewrite nodes <= RandomPick(network, num_nodes) modified_nodes <= NegRewrite(nodes) # positive rewrite PosRewrite(modified_nodes) The difference is that, instead of doing the positive rewrite on the whole network, we only run it on the area we have changed in this iteration. Result The run-time is significantly reduced. The nodes are not properly marked and the result is different from ALTERSEQ.","title":"Logic Synthesis Research Summary"},{"location":"Personal/research_summary.html#logic-synthesis-research-summary","text":"","title":"Logic Synthesis Research Summary"},{"location":"Personal/research_summary.html#timeline-overview","text":"Declaration : useful work : not work","title":"Timeline &amp; Overview"},{"location":"Personal/research_summary.html#2019-oct","text":"Read ABC base Code: understand the data structures inside ABC, e.g. Abc_Ntk_t , Abc_Obj_t , Abc_Cut_t , Vec_Ptr_t , ... Read BLIF , AIGER format: understand how to use tools to convert between different format: e.g. aig2aag , aag2aig Read EPFL benchmarks: understand the contest of best depth and best size . Write a python BLIF parser","title":"2019 Oct"},{"location":"Personal/research_summary.html#2019-dec","text":"Read graphviz : visualize the logic network based on dot and graphviz , can be then converted to post script format. Write Idea 1 .","title":"2019 Dec"},{"location":"Personal/research_summary.html#2020-mar","text":"Help Zhou Zhuoer Write Idea 2","title":"2020 Mar"},{"location":"Personal/research_summary.html#2020-jun","text":"Help Mao Xingyun Write Idea 3 Read SAT based algorithms, RL algorithms: understand recent efforts to apply machine learning algorithms on improving logic synthesis. Read Yosys and EPFL LS Library : parse verilog files and convert to BLIF files. learn EPFL toolkits, including Alice , Mockturtles and try ALSO which is implemented based on EPFL toolkit. Moreover, visit Open Source EDA tools on OpenRoad .","title":"2020 Jun"},{"location":"Personal/research_summary.html#2020-jul","text":"Read ABC advanced commands dc2 and dch . understand 3 basic operations in logic synthesis are rewrite , refactor and resub Write Idea 4","title":"2020 Jul"},{"location":"Personal/research_summary.html#2020-oct","text":"Read Stochastic Algorithms studied by Fiser , including paritial network , Cartesian Genetic Programming , and Random Permuation . Read LEKO and LEKU from J. Cong , which studies the optimality of logic synthesis. Write ABC Rewrite Pre-literature Search Report, which concludes the logic synthesis as a combinatorial optimization problem, which includes local optimizations and global scheduling. Write Visualization Engine for Capstone project, which visualize the logic rewriting process.","title":"2020 Oct"},{"location":"Personal/research_summary.html#2020-dec","text":"Write Idea 5 Read GNN algorithms","title":"2020 Dec"},{"location":"Personal/research_summary.html#2021-jan","text":"Read verification algorithms. understand how to combine simulation and SAT solver and the relationship between logic synthesis and verifications. Read Fanout Optimization Related Works. including TechMap Algorithms , Gate Duplciation , Buffer Insersion , and Rewiring algorithms. Help Huang Shen","title":"2021 Jan"},{"location":"Personal/research_summary.html#2021-apr","text":"Write Idea 6","title":"2021 Apr"},{"location":"Personal/research_summary.html#2021-jun","text":"","title":"2021 Jun"},{"location":"Personal/research_summary.html#problems-solutions-results","text":"Problem 1: Fanout Optimization with strict limit (2019.10 - 2020.03) Develop an algorithm to fix fanout overload in LUT network. The fanout limit for emerging circuit is low but current designs have large fanuot nodes, e.g. dec_depth_2018 's maximum fanout number is 64. Keywords: fanout Solution: convert local LUT network \\(N\\) to AIG \\(N'\\) . fix fanout number using buffer or duplication, get \\(N''\\) map to LUTs \\(N'''\\) Results: our method works on the network with small fanouts local network (MFFC) is usually big, sometimes the local extracted network is equivalent to the whole network. ABC mapper will run redundancy removal and revert our operations to fix fanout overload. Problem 2: Fanout Optimization with soft limit (2020.03 - 2020.04) Develop an algorithm to reduce the fanout delay on the critical path. The delay model is given by \\(Delay(G) = A+B\\times fanout(G))\\) . Keywords: fanout, delay optimization Solution: MFS (2020.03) Modify the target function in ABC mfs , find the new divisors to represent a function and optimize the fanout distribution upon nodes. On the other hand, if the area is optimized during mfs , new nodes are added to duplicate the high fanout nodes. mfs pseudocode for node in network: for n_old in node.fanin: n_new <= TryResub(node, n_old) if fanout(n_old) < fanout (n_new): Update(node): n_old => n_new duplication pseudocode _ntk <= MFS(ntk) area_saved <= node_num(ntk)-node_num(_ntk) while area_saved > 0: Duplicate(highest_fanout_node) area_saved-- Result: 18% TFCP reduction on average (on selected EPFL benchmarks, whose network is large enough and has room of improvement.) all of the reduction are caused by the area optimization and the duplication, but not our \"modified\" mfs method. Update: critical path update & larger window size (2020.11) Improve the duplication and mfs performance by updating the critical path after each network rewriting. Manually set the hyperparameters of ABC mfs function and increase the default window size of mfs while area optimization and rewiring. Result 28% (previous +10%) TFP reduction on average. Update: refine rewiring criteria & enable area increasement (2021.04) write the project ReFO and provide more options to the user. optimize the rewiring criteria and avoid making negative fanout changes to the network. Result 35% Delay Optimization ( \\(1+0.2\\times fanout(G)\\) model) without Area increase on average. Problem 3: Improve ABC rewrite performance on area optimization (2021.03 - present) The target of ABC rewrite is to reduce the AND nodes number in the AIG without increasing the level of it. There are several aspects that could be potentially optimized (sub-problems): 1. the combination of rewrite , refactor , resub . 2. the node traversal order and rewrite task scheduling of rewrite function. 3. the local 4-feasible cut rewrite machanism. Note that the 3 problems are listed from the upper level to the lower level. keywords: AIG, area optimization Solution: optimize the combination of rewrite , refactor , resub -- Priority Scheduler (2020.08) keywords: priority scheduler Add a filter the operation procedure and make decision for each node if it should be operated based on the evaluation result. priority scheduler pseudocode seq <= compress2rs sequence for op in seq: next_op <= op.next for node in network: gain <= EvalGain(op, node)-EvalGain(next_op, node) if gain > 0: Run(op, node) EvalGain(op, node) means try operation op (one of the rewrite , refactor , rewrite ) on the node , and return the gain (node number reduction for area optimization) can get from this operation. We only run operation on this node if and only if the current operation is better than the later operation. Result 0.1% area improvement on the EPFL benchmark hard to decide which solution is better (although the target does not consider level, ABC rewrite is able to reduce the delay. Sometime our area is better but delay is worse). Solution: the node traversal order and rewrite task scheduling of rewrite function (2020.03 - 2020.08) keywords: simulated annealing Modify the traversal order and rewrite update decision making by including a simulated annealing process into the rewrite engine. simulated annealing pseudocode T <= T_start # initial temperature while T > T_end: node <= RandomPick(network) op <= RandomPick({rewrite,refactor,resub}) gain <= Eval(op, node) with exp(gain/T) as probability: Run(op, node) T <= CoolDown(T) Result converge to the same result no matter how we set the parameters. the probability of getting \"coincidence\" that \"accidently\" negatively change the neighbour nodes in the correct way is extremely low (the probability is equal to evolutionary algorithms like CGP). do not have enough theoretical proofs of our negative moves. Update: ALTERSEQ (2020.05) keywords: alternately In order to increase the probability of getting useful negative rewriting and make more changes to the current state, we alternately run Negative rewriting and Positive rewriting, which is similar to the Espresso expand and reduce operations. ALTERNEQ pseudocode iter <= num_iterations # set the number of iterations for iter times: # negative rewrite nodes <= RandomPick(network, num_nodes) NegRewrite(nodes) # positive rewrite PosRewrite(network) Reuslt found some useful negative rewrite on one trail but the probability is still low. the runtime is long due to the large iteration numbers (we have to make sure the value of num_nodes*num_iterations is about the number of nodes we have) Update: QUICKSEQ (2020.06) keywords: quick To accelarate previous method, we develop a faster script. QUICKSEQ pseudocode iter <= num_iterations # set the number of iterations for iter times: # negative rewrite nodes <= RandomPick(network, num_nodes) modified_nodes <= NegRewrite(nodes) # positive rewrite PosRewrite(modified_nodes) The difference is that, instead of doing the positive rewrite on the whole network, we only run it on the area we have changed in this iteration. Result The run-time is significantly reduced. The nodes are not properly marked and the result is different from ALTERSEQ.","title":"Problems, Solutions, &amp; Results"}]}